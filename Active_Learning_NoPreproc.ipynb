{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.8/site-packages (0.18.5)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.8/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.8/site-packages (from wandb) (4.2.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.12.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (2.32.2)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (2.17.0)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.8/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from wandb) (65.6.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /opt/conda/lib/python3.8/site-packages (from wandb) (4.7.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.8/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "# import wandb\n",
    "\n",
    "### GPU Setting ###\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda:0\" if USE_CUDA else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.8/site-packages (24.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#기본 베이스 코드.\\n\\n### Custom Dataset ###\\nclass CUB2011(Dataset):\\n    def __init__(self, transform, mode='train'):\\n        self.transform = transform\\n        self.mode = mode\\n        base_path = 'datasets'  # 기본 경로 설정\\n\\n        if self.mode == 'train':\\n            folder_path = os.path.join(base_path, 'train')\\n        elif self.mode == 'valid':\\n            folder_path = os.path.join(base_path, 'valid')\\n        elif self.mode == 'test':\\n            folder_path = os.path.join(base_path, 'test')\\n\\n        # 디렉토리 제외하고 파일만 리스트에 포함\\n        self.image_folder = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\\n\\n    def __len__(self):\\n        return len(self.image_folder)\\n\\n    def __getitem__(self, idx):\\n        img_path = self.image_folder[idx]\\n        img = Image.open(os.path.join('datasets/', self.mode, img_path)).convert('RGB')\\n        img = self.transform(img)\\n\\n        # 라벨을 이미지 파일명에서 추출 (예: 파일명이 'image_3.jpg'일 때 라벨은 3)\\n        label = img_path.split('_')[-1].split('.')[0]\\n        label = int(label)\\n        return img, label\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#기본 베이스 코드.\n",
    "\n",
    "### Custom Dataset ###\n",
    "class CUB2011(Dataset):\n",
    "    def __init__(self, transform, mode='train'):\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        base_path = 'datasets'  # 기본 경로 설정\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            folder_path = os.path.join(base_path, 'train')\n",
    "        elif self.mode == 'valid':\n",
    "            folder_path = os.path.join(base_path, 'valid')\n",
    "        elif self.mode == 'test':\n",
    "            folder_path = os.path.join(base_path, 'test')\n",
    "\n",
    "        # 디렉토리 제외하고 파일만 리스트에 포함\n",
    "        self.image_folder = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_folder)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_folder[idx]\n",
    "        img = Image.open(os.path.join('datasets/', self.mode, img_path)).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "\n",
    "        # 라벨을 이미지 파일명에서 추출 (예: 파일명이 'image_3.jpg'일 때 라벨은 3)\n",
    "        label = img_path.split('_')[-1].split('.')[0]\n",
    "        label = int(label)\n",
    "        return img, label\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'CUB_200_2011_repackage_class50.zip' 파일이 'CUB_200_2011_class50' 폴더에 성공적으로 압축 해제되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# 압축 파일과 압축 해제 경로\n",
    "zip_file_path = 'CUB_200_2011_repackage_class50.zip'\n",
    "extract_to = 'CUB_200_2011_class50'\n",
    "\n",
    "# 폴더가 존재하지 않으면 생성\n",
    "os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "# 압축 해제\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to)\n",
    "\n",
    "print(f\"'{zip_file_path}' 파일이 '{extract_to}' 폴더에 성공적으로 압축 해제되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 Schema 변형, 예시 데이터 증강 \n",
    "- dataset 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custom Dataset ###\n",
    "class CUB2011(Dataset):\n",
    "    def __init__(self, transform, mode='train'):\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        base_path = 'datasets'  # 기본 경로 설정\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            folder_path = os.path.join(base_path, 'train')\n",
    "        elif self.mode == 'valid':\n",
    "            folder_path = os.path.join(base_path, 'valid')\n",
    "        elif self.mode == 'test':\n",
    "            folder_path = os.path.join(base_path, 'test')\n",
    "\n",
    "        # 디렉토리 제외하고 파일만 리스트에 포함\n",
    "        self.image_folder = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_folder)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_folder[idx]\n",
    "        img = Image.open(os.path.join('datasets/', self.mode, img_path)).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "\n",
    "        # 라벨을 이미지 파일명에서 추출 (예: 파일명이 'image_3.jpg'일 때 라벨은 3)\n",
    "        label = img_path.split('_')[-1].split('.')[0]\n",
    "        label = int(label)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 증강\n",
    "- 크기 448 by 448 고정\n",
    "- 무작위 회전\n",
    "- 무작위 크롭\n",
    "- 좌우 반전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# 데이터 변환 정의\n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.Resize((448, 448)),              # 크기 조정\n",
    "    transforms.RandomRotation(15),              # 무작위 회전\n",
    "    transforms.RandomResizedCrop(448, scale=(0.8, 1.0)),  # 무작위 크롭\n",
    "    transforms.RandomHorizontalFlip(p=0.5),     # 좌우 반전\n",
    "    transforms.ToTensor(),                      # 텐서 변환\n",
    "])\n",
    "\n",
    "transforms_valtest = transforms.Compose([\n",
    "    transforms.Resize((448, 448)),              # 크기 조정\n",
    "    transforms.ToTensor(),                      # 텐서 변환\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of each dataset: 2360 296 298\n",
      "Loaded combined dataloader\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "# 기본 Train 데이터셋\n",
    "org_train_set = CUB2011(mode='train', transform=transforms_valtest)  # 기본 크기 조정만 적용\n",
    "\n",
    "# 증강된 Train 데이터셋\n",
    "transformed_train_set = CUB2011(mode='train', transform=transforms_train)\n",
    "\n",
    "# 기본 데이터셋과 증강 데이터셋을 결합\n",
    "train_set = ConcatDataset([org_train_set, transformed_train_set])\n",
    "\n",
    "# Validation 및 Test 데이터셋\n",
    "val_set = CUB2011(mode='valid', transform=transforms_valtest)\n",
    "test_set = CUB2011(mode='test', transform=transforms_valtest)\n",
    "\n",
    "# 데이터셋 크기 확인\n",
    "print('Num of each dataset:', len(org_train_set), len(val_set), len(test_set))\n",
    "\n",
    "print(\"Loaded combined dataloader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model List 선정\n",
    "\"\"\"\n",
    "1. EfficientNet_V2_M_Weights.IMAGENET1K_V1\n",
    "2. ResNet50_Weights.IMAGENET1K_V2 : parameter수가 2배 차이나지만, 성능은 6퍼센트 높다\n",
    "3. RegNet_Y_1_6GF_Weights_IMAGENET1K_V2 : ResNet18 parameter 수와 같은데 성능이 6퍼센트 높다.\n",
    "4. ConvNext_Tiny_Weights.IMAGENET1K_V1 \n",
    "\"\"\"\n",
    "model_names = {\n",
    "    \"EfficientNet_V2_M\": models.efficientnet_v2_m,\n",
    "    \"ResNet50\": models.resnet50,\n",
    "    \"RegNet_Y_1_6GF\": models.regnet_y_1_6gf,\n",
    "    \"ConvNeXt_Tiny\": models.convnext_tiny\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "### Train/Evaluation with wandb logging ###\n",
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for i, (image, target) in enumerate(train_loader):\n",
    "        image, target = image.to(DEVICE), target.to(DEVICE)\n",
    "        output = model(image)\n",
    "        optimizer.zero_grad()\n",
    "        train_loss = F.cross_entropy(output, target).to(DEVICE)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 매 10번째 배치마다 로그 남기기\n",
    "        if i % 10 == 0:\n",
    "            print(f'Train Epoch : {epoch} [{i}/{len(train_loader)}]\\tLoss: {train_loss.item(): .6f}')\n",
    "    return train_loss\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (image, target) in enumerate(val_loader):\n",
    "            image, target = image.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(image)\n",
    "            eval_loss += F.cross_entropy(output, target, reduction= 'sum').item()\n",
    "            pred = output.argmax(1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    eval_loss /= len(val_loader.dataset)\n",
    "    eval_accuracy = 100 * correct / len(val_loader.dataset)\n",
    "\n",
    "    # validation 결과 wandb에 기록\n",
    "    wandb.log({\"val_loss\": eval_loss, \"val_accuracy\": eval_accuracy})\n",
    "    return eval_loss, eval_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: kq56ju9r\n",
      "Sweep URL: https://wandb.ai/gachon-university-fdslab/resnet18/sweeps/kq56ju9r\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터 스윕 설정\n",
    "sweep_config = {\n",
    "    'name' : 'No-Preprocess',\n",
    "    'method': 'grid',\n",
    "    'metric': {\n",
    "        'name': 'val_accuracy',  # 최적화할 메트릭\n",
    "        'goal': 'maximize'  # 성능을 최대화하려는 목표\n",
    "    },\n",
    "    'parameters': {\n",
    "        'model_name': {\n",
    "            'values': list(model_names.keys())\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'values': [0.01, 0.001, 0.0001]  # 실험할 learning rate\n",
    "        },\n",
    "        'epochs': {\n",
    "            'value': 50\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [16, 32, 64]  # 실험할 배치 사이즈\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values': ['sgd', 'rmsprop', 'adam']  # 사용할 optimizer\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Sweep 생성\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"resnet18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 평가 함수\n",
    "def train_and_evaluate(config=None):\n",
    "    # wandb config 설정\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "\n",
    "        # 모델 로드\n",
    "        print(f\"\")\n",
    "        model_func = model_names[config.model_name]\n",
    "        model = model_func(pretrained=True)\n",
    "        \n",
    "        # Transfer learning: 마지막 레이어 수정\n",
    "        if hasattr(model, 'fc'):\n",
    "            num_features = model.fc.in_features\n",
    "            model.fc = nn.Linear(num_features, 50)\n",
    "        else:\n",
    "            num_features = model.classifier[1].in_features\n",
    "            model.classifier[1] = nn.Linear(num_features, 50)\n",
    "\n",
    "        # 모델 파라미터 설정\n",
    "        params_to_update = []\n",
    "        for param_name, param in model.named_parameters():\n",
    "            if 'fc' in param_name or 'classifier.1' in param_name:\n",
    "                param.requires_grad = True\n",
    "                params_to_update.append(param)\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        model.to(DEVICE)\n",
    "        \n",
    "        if Preprocessing is True :\n",
    "            train_loader = DataLoader(train_set, batch_size=config.batch_size, shuffle=True)\n",
    "        else :\n",
    "            train_loader = DataLoader(org_train_set, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "        val_loader = DataLoader(val_set, batch_size=config.batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_set, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "        # 옵티마이저 정의\n",
    "        if config.optimizer == 'adam':\n",
    "            optimizer = optim.Adam(params_to_update, lr=config.learning_rate)\n",
    "        elif config.optimizer == 'sgd':\n",
    "            optimizer = optim.SGD(params_to_update, lr=config.learning_rate)\n",
    "        elif config.optimizer == 'rmsprop':\n",
    "            optimizer = optim.RMSprop(params_to_update, lr=config.learning_rate)\n",
    "        \n",
    "        best_val_accuracy = 0\n",
    "        for epoch in range(config.epochs):\n",
    "            train_loss = train(model, train_loader, optimizer, epoch)\n",
    "            val_loss, val_accuracy = evaluate(model, val_loader)\n",
    "\n",
    "            # 최고 성능 모델 저장\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                torch.save(model.state_dict(), f\"./best_model_{config.model_name}.pth\")\n",
    "                print(f'Best model saved for {config.model_name} with accuracy: {best_val_accuracy:.4f}')\n",
    "\n",
    "            # Log epoch results\n",
    "            wandb.log({\"best_val_accuracy\": best_val_accuracy, \"val_loss\": val_loss, \"epoch\": epoch})\n",
    "\n",
    "        # 최종 테스트 평가\n",
    "        test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "        print(f'[FINAL] {config.model_name} Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}')\n",
    "        wandb.log({\"test_loss\": test_loss, \"test_accuracy\": test_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rb3ueusp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_name: EfficientNet_V2_M\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20241029_042440-rb3ueusp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gachon-university-fdslab/resnet18/runs/rb3ueusp' target=\"_blank\">kind-sweep-1</a></strong> to <a href='https://wandb.ai/gachon-university-fdslab/resnet18' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/gachon-university-fdslab/resnet18/sweeps/kq56ju9r' target=\"_blank\">https://wandb.ai/gachon-university-fdslab/resnet18/sweeps/kq56ju9r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gachon-university-fdslab/resnet18' target=\"_blank\">https://wandb.ai/gachon-university-fdslab/resnet18</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/gachon-university-fdslab/resnet18/sweeps/kq56ju9r' target=\"_blank\">https://wandb.ai/gachon-university-fdslab/resnet18/sweeps/kq56ju9r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gachon-university-fdslab/resnet18/runs/rb3ueusp' target=\"_blank\">https://wandb.ai/gachon-university-fdslab/resnet18/runs/rb3ueusp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch : 0 [0/148]\tLoss:  3.924208\n",
      "Train Epoch : 0 [10/148]\tLoss:  3.883040\n",
      "Train Epoch : 0 [20/148]\tLoss:  3.895839\n",
      "Train Epoch : 0 [30/148]\tLoss:  3.813031\n",
      "Train Epoch : 0 [40/148]\tLoss:  3.814166\n",
      "Train Epoch : 0 [50/148]\tLoss:  3.736363\n",
      "Train Epoch : 0 [60/148]\tLoss:  3.631399\n",
      "Train Epoch : 0 [70/148]\tLoss:  3.578503\n",
      "Train Epoch : 0 [80/148]\tLoss:  3.431796\n",
      "Train Epoch : 0 [90/148]\tLoss:  3.619381\n",
      "Train Epoch : 0 [100/148]\tLoss:  3.449720\n",
      "Train Epoch : 0 [110/148]\tLoss:  3.417991\n",
      "Train Epoch : 0 [120/148]\tLoss:  3.449693\n",
      "Train Epoch : 0 [130/148]\tLoss:  3.300104\n",
      "Train Epoch : 0 [140/148]\tLoss:  3.088228\n",
      "Best model saved for EfficientNet_V2_M with accuracy: 58.7838\n",
      "Train Epoch : 1 [0/148]\tLoss:  3.054585\n",
      "Train Epoch : 1 [10/148]\tLoss:  3.096778\n",
      "Train Epoch : 1 [20/148]\tLoss:  2.896196\n",
      "Train Epoch : 1 [30/148]\tLoss:  2.875313\n",
      "Train Epoch : 1 [40/148]\tLoss:  2.921327\n",
      "Train Epoch : 1 [50/148]\tLoss:  2.510246\n",
      "Train Epoch : 1 [60/148]\tLoss:  2.522558\n",
      "Train Epoch : 1 [70/148]\tLoss:  2.357203\n",
      "Train Epoch : 1 [80/148]\tLoss:  2.566297\n",
      "Train Epoch : 1 [90/148]\tLoss:  2.296144\n",
      "Train Epoch : 1 [100/148]\tLoss:  2.223640\n",
      "Train Epoch : 1 [110/148]\tLoss:  1.898229\n",
      "Train Epoch : 1 [120/148]\tLoss:  1.811887\n",
      "Train Epoch : 1 [130/148]\tLoss:  2.002646\n",
      "Train Epoch : 1 [140/148]\tLoss:  1.999228\n",
      "Best model saved for EfficientNet_V2_M with accuracy: 72.9730\n",
      "Train Epoch : 2 [0/148]\tLoss:  1.884501\n",
      "Train Epoch : 2 [10/148]\tLoss:  1.663254\n",
      "Train Epoch : 2 [20/148]\tLoss:  1.168098\n",
      "Train Epoch : 2 [30/148]\tLoss:  1.432286\n",
      "Train Epoch : 2 [40/148]\tLoss:  1.273817\n",
      "Train Epoch : 2 [50/148]\tLoss:  1.610311\n",
      "Train Epoch : 2 [60/148]\tLoss:  1.516021\n",
      "Train Epoch : 2 [70/148]\tLoss:  1.629477\n",
      "Train Epoch : 2 [80/148]\tLoss:  1.527522\n",
      "Train Epoch : 2 [90/148]\tLoss:  0.934067\n",
      "Train Epoch : 2 [100/148]\tLoss:  1.492741\n",
      "Train Epoch : 2 [110/148]\tLoss:  1.180903\n",
      "Train Epoch : 2 [120/148]\tLoss:  0.933046\n",
      "Train Epoch : 2 [130/148]\tLoss:  1.132426\n",
      "Train Epoch : 2 [140/148]\tLoss:  1.046571\n",
      "Best model saved for EfficientNet_V2_M with accuracy: 81.4189\n",
      "Train Epoch : 3 [0/148]\tLoss:  0.961589\n",
      "Train Epoch : 3 [10/148]\tLoss:  1.297713\n",
      "Train Epoch : 3 [20/148]\tLoss:  1.352207\n",
      "Train Epoch : 3 [30/148]\tLoss:  1.121341\n",
      "Train Epoch : 3 [40/148]\tLoss:  0.808934\n",
      "Train Epoch : 3 [50/148]\tLoss:  1.025771\n",
      "Train Epoch : 3 [60/148]\tLoss:  0.884507\n",
      "Train Epoch : 3 [70/148]\tLoss:  1.536083\n",
      "Train Epoch : 3 [80/148]\tLoss:  0.931944\n",
      "Train Epoch : 3 [90/148]\tLoss:  0.972756\n",
      "Train Epoch : 3 [100/148]\tLoss:  0.770867\n",
      "Train Epoch : 3 [110/148]\tLoss:  1.382661\n",
      "Train Epoch : 3 [120/148]\tLoss:  0.935347\n",
      "Train Epoch : 3 [130/148]\tLoss:  0.681897\n",
      "Train Epoch : 3 [140/148]\tLoss:  0.851386\n",
      "Best model saved for EfficientNet_V2_M with accuracy: 88.5135\n",
      "Train Epoch : 4 [0/148]\tLoss:  0.429558\n",
      "Train Epoch : 4 [10/148]\tLoss:  0.768571\n",
      "Train Epoch : 4 [20/148]\tLoss:  0.857840\n",
      "Train Epoch : 4 [30/148]\tLoss:  1.034150\n",
      "Train Epoch : 4 [40/148]\tLoss:  0.535967\n",
      "Train Epoch : 4 [50/148]\tLoss:  0.502311\n",
      "Train Epoch : 4 [60/148]\tLoss:  0.720878\n",
      "Train Epoch : 4 [70/148]\tLoss:  0.748500\n",
      "Train Epoch : 4 [80/148]\tLoss:  0.425831\n",
      "Train Epoch : 4 [90/148]\tLoss:  0.691795\n",
      "Train Epoch : 4 [100/148]\tLoss:  0.696544\n",
      "Train Epoch : 4 [110/148]\tLoss:  0.636113\n",
      "Train Epoch : 4 [120/148]\tLoss:  0.420042\n",
      "Train Epoch : 4 [130/148]\tLoss:  0.616701\n",
      "Train Epoch : 4 [140/148]\tLoss:  0.804614\n",
      "Best model saved for EfficientNet_V2_M with accuracy: 89.1892\n",
      "Train Epoch : 5 [0/148]\tLoss:  0.693176\n",
      "Train Epoch : 5 [10/148]\tLoss:  0.362458\n",
      "Train Epoch : 5 [20/148]\tLoss:  0.528572\n",
      "Train Epoch : 5 [30/148]\tLoss:  0.413725\n",
      "Train Epoch : 5 [40/148]\tLoss:  0.343035\n",
      "Train Epoch : 5 [50/148]\tLoss:  0.776496\n",
      "Train Epoch : 5 [60/148]\tLoss:  0.388110\n",
      "Train Epoch : 5 [70/148]\tLoss:  0.625539\n",
      "Train Epoch : 5 [80/148]\tLoss:  0.446213\n",
      "Train Epoch : 5 [90/148]\tLoss:  0.496106\n",
      "Train Epoch : 5 [100/148]\tLoss:  0.433094\n",
      "Train Epoch : 5 [110/148]\tLoss:  0.462942\n",
      "Train Epoch : 5 [120/148]\tLoss:  0.413242\n",
      "Train Epoch : 5 [130/148]\tLoss:  0.276563\n",
      "Train Epoch : 5 [140/148]\tLoss:  0.438225\n",
      "Best model saved for EfficientNet_V2_M with accuracy: 90.8784\n",
      "Train Epoch : 6 [0/148]\tLoss:  0.654606\n",
      "Train Epoch : 6 [10/148]\tLoss:  0.520363\n",
      "Train Epoch : 6 [20/148]\tLoss:  0.573893\n",
      "Train Epoch : 6 [30/148]\tLoss:  0.720135\n",
      "Train Epoch : 6 [40/148]\tLoss:  0.514515\n",
      "Train Epoch : 6 [50/148]\tLoss:  0.487961\n",
      "Train Epoch : 6 [60/148]\tLoss:  0.783147\n",
      "Train Epoch : 6 [70/148]\tLoss:  0.635368\n",
      "Train Epoch : 6 [80/148]\tLoss:  0.309174\n",
      "Train Epoch : 6 [90/148]\tLoss:  0.338699\n",
      "Train Epoch : 6 [100/148]\tLoss:  0.704560\n",
      "Train Epoch : 6 [110/148]\tLoss:  0.461675\n",
      "Train Epoch : 6 [120/148]\tLoss:  0.662186\n",
      "Train Epoch : 6 [130/148]\tLoss:  0.342248\n",
      "Train Epoch : 6 [140/148]\tLoss:  0.439348\n",
      "Best model saved for EfficientNet_V2_M with accuracy: 91.2162\n",
      "Train Epoch : 7 [0/148]\tLoss:  0.364502\n",
      "Train Epoch : 7 [10/148]\tLoss:  0.402237\n",
      "Train Epoch : 7 [20/148]\tLoss:  0.431045\n",
      "Train Epoch : 7 [30/148]\tLoss:  0.423456\n",
      "Train Epoch : 7 [40/148]\tLoss:  0.538137\n",
      "Train Epoch : 7 [50/148]\tLoss:  0.397406\n",
      "Train Epoch : 7 [60/148]\tLoss:  0.264282\n",
      "Train Epoch : 7 [70/148]\tLoss:  0.393902\n",
      "Train Epoch : 7 [80/148]\tLoss:  0.294483\n",
      "Train Epoch : 7 [90/148]\tLoss:  0.287859\n",
      "Train Epoch : 7 [100/148]\tLoss:  0.317224\n",
      "Train Epoch : 7 [110/148]\tLoss:  0.357050\n",
      "Train Epoch : 7 [120/148]\tLoss:  0.336562\n",
      "Train Epoch : 7 [130/148]\tLoss:  0.186957\n",
      "Train Epoch : 7 [140/148]\tLoss:  0.378733\n",
      "Best model saved for EfficientNet_V2_M with accuracy: 93.2432\n",
      "Train Epoch : 8 [0/148]\tLoss:  0.379621\n",
      "Train Epoch : 8 [10/148]\tLoss:  0.354859\n",
      "Train Epoch : 8 [20/148]\tLoss:  0.458543\n",
      "Train Epoch : 8 [30/148]\tLoss:  0.411215\n",
      "Train Epoch : 8 [40/148]\tLoss:  0.361508\n",
      "Train Epoch : 8 [50/148]\tLoss:  0.771709\n",
      "Train Epoch : 8 [60/148]\tLoss:  0.271263\n",
      "Train Epoch : 8 [70/148]\tLoss:  0.297173\n",
      "Train Epoch : 8 [80/148]\tLoss:  0.560941\n",
      "Train Epoch : 8 [90/148]\tLoss:  0.282921\n",
      "Train Epoch : 8 [100/148]\tLoss:  0.280368\n",
      "Train Epoch : 8 [110/148]\tLoss:  0.385813\n",
      "Train Epoch : 8 [120/148]\tLoss:  0.315620\n",
      "Train Epoch : 8 [130/148]\tLoss:  0.276230\n",
      "Train Epoch : 8 [140/148]\tLoss:  0.236481\n",
      "Best model saved for EfficientNet_V2_M with accuracy: 93.5811\n",
      "Train Epoch : 9 [0/148]\tLoss:  0.298919\n",
      "Train Epoch : 9 [10/148]\tLoss:  0.313019\n",
      "Train Epoch : 9 [20/148]\tLoss:  0.120154\n",
      "Train Epoch : 9 [30/148]\tLoss:  0.348690\n",
      "Train Epoch : 9 [40/148]\tLoss:  0.438653\n",
      "Train Epoch : 9 [50/148]\tLoss:  0.422926\n",
      "Train Epoch : 9 [60/148]\tLoss:  0.345206\n",
      "Train Epoch : 9 [70/148]\tLoss:  0.363048\n",
      "Train Epoch : 9 [80/148]\tLoss:  0.360857\n",
      "Train Epoch : 9 [90/148]\tLoss:  0.204873\n",
      "Train Epoch : 9 [100/148]\tLoss:  0.370439\n",
      "Train Epoch : 9 [110/148]\tLoss:  0.274001\n",
      "Train Epoch : 9 [120/148]\tLoss:  0.260512\n",
      "Train Epoch : 9 [130/148]\tLoss:  0.345418\n",
      "Train Epoch : 9 [140/148]\tLoss:  0.521945\n",
      "Best model saved for EfficientNet_V2_M with accuracy: 94.2568\n",
      "Train Epoch : 10 [0/148]\tLoss:  0.449029\n",
      "Train Epoch : 10 [10/148]\tLoss:  0.171873\n",
      "Train Epoch : 10 [20/148]\tLoss:  0.255367\n",
      "Train Epoch : 10 [30/148]\tLoss:  0.158042\n",
      "Train Epoch : 10 [40/148]\tLoss:  0.072109\n",
      "Train Epoch : 10 [50/148]\tLoss:  0.297108\n",
      "Train Epoch : 10 [60/148]\tLoss:  0.693576\n",
      "Train Epoch : 10 [70/148]\tLoss:  0.315734\n",
      "Train Epoch : 10 [80/148]\tLoss:  0.354978\n",
      "Train Epoch : 10 [90/148]\tLoss:  0.354189\n",
      "Train Epoch : 10 [100/148]\tLoss:  0.130147\n",
      "Train Epoch : 10 [110/148]\tLoss:  0.196008\n",
      "Train Epoch : 10 [120/148]\tLoss:  0.171641\n",
      "Train Epoch : 10 [130/148]\tLoss:  0.367741\n",
      "Train Epoch : 10 [140/148]\tLoss:  0.274855\n",
      "Train Epoch : 11 [0/148]\tLoss:  0.547526\n",
      "Train Epoch : 11 [10/148]\tLoss:  0.357831\n",
      "Train Epoch : 11 [20/148]\tLoss:  0.307339\n",
      "Train Epoch : 11 [30/148]\tLoss:  0.136484\n",
      "Train Epoch : 11 [40/148]\tLoss:  0.249943\n",
      "Train Epoch : 11 [50/148]\tLoss:  0.430403\n",
      "Train Epoch : 11 [60/148]\tLoss:  0.312039\n",
      "Train Epoch : 11 [70/148]\tLoss:  0.170492\n",
      "Train Epoch : 11 [80/148]\tLoss:  0.333086\n",
      "Train Epoch : 11 [90/148]\tLoss:  0.323918\n",
      "Train Epoch : 11 [100/148]\tLoss:  0.527376\n",
      "Train Epoch : 11 [110/148]\tLoss:  0.125662\n",
      "Train Epoch : 11 [120/148]\tLoss:  0.187311\n",
      "Train Epoch : 11 [130/148]\tLoss:  0.156605\n",
      "Train Epoch : 11 [140/148]\tLoss:  0.513598\n",
      "Train Epoch : 12 [0/148]\tLoss:  0.322999\n",
      "Train Epoch : 12 [10/148]\tLoss:  0.299124\n",
      "Train Epoch : 12 [20/148]\tLoss:  0.214425\n",
      "Train Epoch : 12 [30/148]\tLoss:  0.083469\n",
      "Train Epoch : 12 [40/148]\tLoss:  0.195561\n",
      "Train Epoch : 12 [50/148]\tLoss:  0.217644\n",
      "Train Epoch : 12 [60/148]\tLoss:  0.124449\n",
      "Train Epoch : 12 [70/148]\tLoss:  0.313990\n",
      "Train Epoch : 12 [80/148]\tLoss:  0.533666\n",
      "Train Epoch : 12 [90/148]\tLoss:  0.418191\n",
      "Train Epoch : 12 [100/148]\tLoss:  0.758280\n",
      "Train Epoch : 12 [110/148]\tLoss:  0.408188\n",
      "Train Epoch : 12 [120/148]\tLoss:  0.353966\n",
      "Train Epoch : 12 [130/148]\tLoss:  0.396505\n",
      "Train Epoch : 12 [140/148]\tLoss:  0.306593\n",
      "Best model saved for EfficientNet_V2_M with accuracy: 94.5946\n",
      "Train Epoch : 13 [0/148]\tLoss:  0.173522\n",
      "Train Epoch : 13 [10/148]\tLoss:  0.139714\n",
      "Train Epoch : 13 [20/148]\tLoss:  0.261235\n",
      "Train Epoch : 13 [30/148]\tLoss:  0.101159\n",
      "Train Epoch : 13 [40/148]\tLoss:  0.118845\n",
      "Train Epoch : 13 [50/148]\tLoss:  0.084633\n",
      "Train Epoch : 13 [60/148]\tLoss:  0.197868\n",
      "Train Epoch : 13 [70/148]\tLoss:  0.229118\n",
      "Train Epoch : 13 [80/148]\tLoss:  0.229535\n",
      "Train Epoch : 13 [90/148]\tLoss:  0.221436\n",
      "Train Epoch : 13 [100/148]\tLoss:  0.303347\n",
      "Train Epoch : 13 [110/148]\tLoss:  0.165549\n",
      "Train Epoch : 13 [120/148]\tLoss:  0.142003\n",
      "Train Epoch : 13 [130/148]\tLoss:  0.569451\n",
      "Train Epoch : 13 [140/148]\tLoss:  0.294053\n",
      "Train Epoch : 14 [0/148]\tLoss:  0.133818\n",
      "Train Epoch : 14 [10/148]\tLoss:  0.262679\n",
      "Train Epoch : 14 [20/148]\tLoss:  0.159654\n",
      "Train Epoch : 14 [30/148]\tLoss:  0.152116\n",
      "Train Epoch : 14 [40/148]\tLoss:  0.129312\n",
      "Train Epoch : 14 [50/148]\tLoss:  0.320317\n",
      "Train Epoch : 14 [60/148]\tLoss:  0.084805\n",
      "Train Epoch : 14 [70/148]\tLoss:  0.038760\n",
      "Train Epoch : 14 [80/148]\tLoss:  0.160225\n",
      "Train Epoch : 14 [90/148]\tLoss:  0.235141\n",
      "Train Epoch : 14 [100/148]\tLoss:  0.457732\n",
      "Train Epoch : 14 [110/148]\tLoss:  0.167603\n",
      "Train Epoch : 14 [120/148]\tLoss:  0.300390\n",
      "Train Epoch : 14 [130/148]\tLoss:  0.287097\n",
      "Train Epoch : 14 [140/148]\tLoss:  0.114441\n",
      "Train Epoch : 15 [0/148]\tLoss:  0.264319\n",
      "Train Epoch : 15 [10/148]\tLoss:  0.195038\n",
      "Train Epoch : 15 [20/148]\tLoss:  0.108971\n",
      "Train Epoch : 15 [30/148]\tLoss:  0.399576\n",
      "Train Epoch : 15 [40/148]\tLoss:  0.328323\n",
      "Train Epoch : 15 [50/148]\tLoss:  0.157355\n",
      "Train Epoch : 15 [60/148]\tLoss:  0.615419\n",
      "Train Epoch : 15 [70/148]\tLoss:  0.300905\n",
      "Train Epoch : 15 [80/148]\tLoss:  0.261511\n",
      "Train Epoch : 15 [90/148]\tLoss:  0.141487\n",
      "Train Epoch : 15 [100/148]\tLoss:  0.067140\n",
      "Train Epoch : 15 [110/148]\tLoss:  0.229571\n",
      "Train Epoch : 15 [120/148]\tLoss:  0.166551\n",
      "Train Epoch : 15 [130/148]\tLoss:  0.148643\n",
      "Train Epoch : 15 [140/148]\tLoss:  0.118409\n",
      "Train Epoch : 16 [0/148]\tLoss:  0.289056\n",
      "Train Epoch : 16 [10/148]\tLoss:  0.189859\n",
      "Train Epoch : 16 [20/148]\tLoss:  0.398235\n",
      "Train Epoch : 16 [30/148]\tLoss:  0.224237\n",
      "Train Epoch : 16 [40/148]\tLoss:  0.155041\n",
      "Train Epoch : 16 [50/148]\tLoss:  0.186676\n",
      "Train Epoch : 16 [60/148]\tLoss:  0.191002\n",
      "Train Epoch : 16 [70/148]\tLoss:  0.284673\n",
      "Train Epoch : 16 [80/148]\tLoss:  0.242317\n",
      "Train Epoch : 16 [90/148]\tLoss:  0.106353\n",
      "Train Epoch : 16 [100/148]\tLoss:  0.086051\n",
      "Train Epoch : 16 [110/148]\tLoss:  0.072782\n",
      "Train Epoch : 16 [120/148]\tLoss:  0.173955\n",
      "Train Epoch : 16 [130/148]\tLoss:  0.058111\n",
      "Train Epoch : 16 [140/148]\tLoss:  0.243676\n",
      "Train Epoch : 17 [0/148]\tLoss:  0.101031\n",
      "Train Epoch : 17 [10/148]\tLoss:  0.059660\n",
      "Train Epoch : 17 [20/148]\tLoss:  0.116033\n",
      "Train Epoch : 17 [30/148]\tLoss:  0.055966\n",
      "Train Epoch : 17 [40/148]\tLoss:  0.207586\n",
      "Train Epoch : 17 [50/148]\tLoss:  0.139378\n",
      "Train Epoch : 17 [60/148]\tLoss:  0.089884\n",
      "Train Epoch : 17 [70/148]\tLoss:  0.115544\n",
      "Train Epoch : 17 [80/148]\tLoss:  0.312004\n",
      "Train Epoch : 17 [90/148]\tLoss:  0.048084\n",
      "Train Epoch : 17 [100/148]\tLoss:  0.060343\n",
      "Train Epoch : 17 [110/148]\tLoss:  0.232507\n",
      "Train Epoch : 17 [120/148]\tLoss:  0.250125\n",
      "Train Epoch : 17 [130/148]\tLoss:  0.184216\n",
      "Train Epoch : 17 [140/148]\tLoss:  0.131945\n",
      "Train Epoch : 18 [0/148]\tLoss:  0.165976\n",
      "Train Epoch : 18 [10/148]\tLoss:  0.075331\n",
      "Train Epoch : 18 [20/148]\tLoss:  0.220022\n",
      "Train Epoch : 18 [30/148]\tLoss:  0.307781\n",
      "Train Epoch : 18 [40/148]\tLoss:  0.294139\n",
      "Train Epoch : 18 [50/148]\tLoss:  0.129397\n",
      "Train Epoch : 18 [60/148]\tLoss:  0.121667\n",
      "Train Epoch : 18 [70/148]\tLoss:  0.181032\n",
      "Train Epoch : 18 [80/148]\tLoss:  0.122787\n",
      "Train Epoch : 18 [90/148]\tLoss:  0.083136\n",
      "Train Epoch : 18 [100/148]\tLoss:  0.133250\n",
      "Train Epoch : 18 [110/148]\tLoss:  0.028083\n",
      "Train Epoch : 18 [120/148]\tLoss:  0.185525\n",
      "Train Epoch : 18 [130/148]\tLoss:  0.249271\n",
      "Train Epoch : 18 [140/148]\tLoss:  0.161706\n",
      "Train Epoch : 19 [0/148]\tLoss:  0.197276\n",
      "Train Epoch : 19 [10/148]\tLoss:  0.082108\n",
      "Train Epoch : 19 [20/148]\tLoss:  0.162807\n",
      "Train Epoch : 19 [30/148]\tLoss:  0.133328\n",
      "Train Epoch : 19 [40/148]\tLoss:  0.154040\n",
      "Train Epoch : 19 [50/148]\tLoss:  0.079373\n",
      "Train Epoch : 19 [60/148]\tLoss:  0.231885\n",
      "Train Epoch : 19 [70/148]\tLoss:  0.096284\n",
      "Train Epoch : 19 [80/148]\tLoss:  0.199770\n",
      "Train Epoch : 19 [90/148]\tLoss:  0.176574\n",
      "Train Epoch : 19 [100/148]\tLoss:  0.658896\n",
      "Train Epoch : 19 [110/148]\tLoss:  0.036216\n",
      "Train Epoch : 19 [120/148]\tLoss:  0.203869\n",
      "Train Epoch : 19 [130/148]\tLoss:  0.150574\n",
      "Train Epoch : 19 [140/148]\tLoss:  0.223756\n",
      "Train Epoch : 20 [0/148]\tLoss:  0.106744\n",
      "Train Epoch : 20 [10/148]\tLoss:  0.181131\n",
      "Train Epoch : 20 [20/148]\tLoss:  0.268851\n",
      "Train Epoch : 20 [30/148]\tLoss:  0.261078\n",
      "Train Epoch : 20 [40/148]\tLoss:  0.108804\n",
      "Train Epoch : 20 [50/148]\tLoss:  0.093793\n",
      "Train Epoch : 20 [60/148]\tLoss:  0.262967\n",
      "Train Epoch : 20 [70/148]\tLoss:  0.084343\n",
      "Train Epoch : 20 [80/148]\tLoss:  0.079874\n",
      "Train Epoch : 20 [90/148]\tLoss:  0.110572\n",
      "Train Epoch : 20 [100/148]\tLoss:  0.065140\n",
      "Train Epoch : 20 [110/148]\tLoss:  0.230062\n",
      "Train Epoch : 20 [120/148]\tLoss:  0.249229\n",
      "Train Epoch : 20 [130/148]\tLoss:  0.367740\n",
      "Train Epoch : 20 [140/148]\tLoss:  0.142910\n",
      "Best model saved for EfficientNet_V2_M with accuracy: 94.9324\n",
      "Train Epoch : 21 [0/148]\tLoss:  0.116457\n",
      "Train Epoch : 21 [10/148]\tLoss:  0.087487\n",
      "Train Epoch : 21 [20/148]\tLoss:  0.066157\n",
      "Train Epoch : 21 [30/148]\tLoss:  0.060075\n",
      "Train Epoch : 21 [40/148]\tLoss:  0.031258\n",
      "Train Epoch : 21 [50/148]\tLoss:  0.444909\n",
      "Train Epoch : 21 [60/148]\tLoss:  0.104118\n",
      "Train Epoch : 21 [70/148]\tLoss:  0.159118\n",
      "Train Epoch : 21 [80/148]\tLoss:  0.162372\n",
      "Train Epoch : 21 [90/148]\tLoss:  0.049007\n",
      "Train Epoch : 21 [100/148]\tLoss:  0.154381\n",
      "Train Epoch : 21 [110/148]\tLoss:  0.170518\n",
      "Train Epoch : 21 [120/148]\tLoss:  0.132442\n",
      "Train Epoch : 21 [130/148]\tLoss:  0.171885\n",
      "Train Epoch : 21 [140/148]\tLoss:  0.156797\n",
      "Train Epoch : 22 [0/148]\tLoss:  0.069208\n",
      "Train Epoch : 22 [10/148]\tLoss:  0.072442\n",
      "Train Epoch : 22 [20/148]\tLoss:  0.114668\n",
      "Train Epoch : 22 [30/148]\tLoss:  0.128037\n",
      "Train Epoch : 22 [40/148]\tLoss:  0.246187\n"
     ]
    }
   ],
   "source": [
    "# Sweep agent \n",
    "Preprocessing = False\n",
    "\n",
    "wandb.agent(sweep_id, train_and_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
